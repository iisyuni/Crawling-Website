{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Web Crawler Menggunakan Python Nama : Iis Yuni Harianti NIM : 160411100086 Semester : 6 Mata kuliah : Pengembangan dan Pencarian Web Dosen Pengampu : Mulaab, S.Si., M.Kom Library yang di butuhkan untuk mengcrawl untuk memulai melakukan crawl pastikan python yang digunakan sudah memiliki library sebagai berikut : BeautifulSoup4 digunakan untuk mengambil data berdasarkan tag html. requests digunakan untuk menganbil satu halaman html. Sastrawi digunakan untuk mengubah kata menjadi kata dasar. numpy digunakan untuk mengolah data numerik diolah menjadi matrik scikit-learn digunakan untuk menyediakan rumus untuk menghitung K-means, Silhouette. from math import log10 import requests from bs4 import BeautifulSoup import sqlite3 from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from Sastrawi.Stemmer.StemmerFactory import StemmerFactory import csv from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score import numpy as np import warnings Note: Untuk menjalankan programnya pastikan terhubung dengan koneksi internet. Agar tidak terjadi error. Untuk database KBBI (KBI.db) pastikan tersimpan dalam satu folder dengan file programnya crawling.py","title":"Pengantar"},{"location":"#web-crawler-menggunakan-python","text":"Nama : Iis Yuni Harianti NIM : 160411100086 Semester : 6 Mata kuliah : Pengembangan dan Pencarian Web Dosen Pengampu : Mulaab, S.Si., M.Kom","title":"Web Crawler Menggunakan Python"},{"location":"#library-yang-di-butuhkan-untuk-mengcrawl","text":"untuk memulai melakukan crawl pastikan python yang digunakan sudah memiliki library sebagai berikut : BeautifulSoup4 digunakan untuk mengambil data berdasarkan tag html. requests digunakan untuk menganbil satu halaman html. Sastrawi digunakan untuk mengubah kata menjadi kata dasar. numpy digunakan untuk mengolah data numerik diolah menjadi matrik scikit-learn digunakan untuk menyediakan rumus untuk menghitung K-means, Silhouette. from math import log10 import requests from bs4 import BeautifulSoup import sqlite3 from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from Sastrawi.Stemmer.StemmerFactory import StemmerFactory import csv from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score import numpy as np import warnings","title":"Library yang di  butuhkan untuk mengcrawl"},{"location":"#note","text":"Untuk menjalankan programnya pastikan terhubung dengan koneksi internet. Agar tidak terjadi error. Untuk database KBBI (KBI.db) pastikan tersimpan dalam satu folder dengan file programnya crawling.py","title":"Note:"},{"location":"Crawling/","text":"Crawling Web Crawler digunakan untuk mengambil data berupa teks, audio, foto bahkan video pada sebuah website. source code untuk mengcrawl data Proses crawling dalam suatu website dimulai dari mendata seluruh url dari website, menelusurinya satu-persatu, kemudian memasukkannya dalam daftar halaman pada indeks search engine, sehingga setiap kali ada perubahan pada website, akan terupdate secara otomatis 1 . src = https://www.kompasiana.com/olahraga/ page = requests . get ( src ) soup = BeautifulSoup ( page . content , html.parser ) artikel = soup . findAll ( class_ = title mt40 ) koneksi = sqlite3 . connect ( db_data.db ) koneksi . execute ( CREATE TABLE if not exists kompasiana (judul TEXT NOT NULL, isi TEXT NOT NULL); ) for i in range ( len ( artikel )): link = artikel [ i ] . find ( a )[ href ] page = requests . get ( link ) soup = BeautifulSoup ( page . content , html.parser ) judul = soup . find ( class_ = title ) . getText () isi = soup . find ( class_ = read-content col-lg-9 col-md-9 col-sm-9 col-xs-9 ) paragraf = isi . findAll ( p ) p = for s in paragraf : p += s . getText () + cek = koneksi . execute ( SELECT * FROM kompasiana where judul=? , ( judul ,)) cek = cek . fetchall () if len ( cek ) == 0 : koneksi . execute ( INSERT INTO kompasiana values (?,?) , ( judul , p )); koneksi . commit () tampil = koneksi . execute ( SELECT * FROM kompasiana ) with open ( data_crawler.csv , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) for i in tampil : employee_writer . writerow ( i ) tampil = koneksi . execute ( SELECT * FROM kompasiana ) isi = [] for row in tampil : isi . append ( row [ 1 ]) #print(row) print ( crawl ) Data yang telah di crawl kemudian di simpan ke dalam database yaitu SQlite Hasil Running : http://www.teknologi-bigdata.com/2016/07/web-crawling-di-era-big-data.html","title":"Crawling"},{"location":"Crawling/#crawling","text":"Web Crawler digunakan untuk mengambil data berupa teks, audio, foto bahkan video pada sebuah website.","title":"Crawling"},{"location":"Crawling/#source-code-untuk-mengcrawl-data","text":"Proses crawling dalam suatu website dimulai dari mendata seluruh url dari website, menelusurinya satu-persatu, kemudian memasukkannya dalam daftar halaman pada indeks search engine, sehingga setiap kali ada perubahan pada website, akan terupdate secara otomatis 1 . src = https://www.kompasiana.com/olahraga/ page = requests . get ( src ) soup = BeautifulSoup ( page . content , html.parser ) artikel = soup . findAll ( class_ = title mt40 ) koneksi = sqlite3 . connect ( db_data.db ) koneksi . execute ( CREATE TABLE if not exists kompasiana (judul TEXT NOT NULL, isi TEXT NOT NULL); ) for i in range ( len ( artikel )): link = artikel [ i ] . find ( a )[ href ] page = requests . get ( link ) soup = BeautifulSoup ( page . content , html.parser ) judul = soup . find ( class_ = title ) . getText () isi = soup . find ( class_ = read-content col-lg-9 col-md-9 col-sm-9 col-xs-9 ) paragraf = isi . findAll ( p ) p = for s in paragraf : p += s . getText () + cek = koneksi . execute ( SELECT * FROM kompasiana where judul=? , ( judul ,)) cek = cek . fetchall () if len ( cek ) == 0 : koneksi . execute ( INSERT INTO kompasiana values (?,?) , ( judul , p )); koneksi . commit () tampil = koneksi . execute ( SELECT * FROM kompasiana ) with open ( data_crawler.csv , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) for i in tampil : employee_writer . writerow ( i ) tampil = koneksi . execute ( SELECT * FROM kompasiana ) isi = [] for row in tampil : isi . append ( row [ 1 ]) #print(row) print ( crawl ) Data yang telah di crawl kemudian di simpan ke dalam database yaitu SQlite","title":"source code untuk mengcrawl data"},{"location":"Crawling/#hasil-running","text":"http://www.teknologi-bigdata.com/2016/07/web-crawling-di-era-big-data.html","title":"Hasil Running :"},{"location":"Silhouette/","text":"Silhouette Silhouette merupakan evaluasi cluster apakah cluster yang digunakan sudah baik apa belum. Metode pengujian yang akan digunakan adalah Silhouette Coefficient. Metode silhouette coefficient merupakan gabungan dari dua metode yaitu metode cohesion yang berfungsi untuk mengukur seberapa dekat relasi antara objek dalam sebuah cluster, dan metode separation yang berfungsi untuk mengukur seberapa jauh sebuah cluster terpisah dengan cluster lain. 1 Tahapan perhitungan silhouette coefficient : Hitung rata-rata jarak objek dengan semua objek lain yang berada di dalam satu cluster dengan persamaan : \u200b Gambar 1 Rumus Menghitung Rata-Rata Objek Dengan Objek Lain Dalam Satu Cluster Hitung rata-rata jarak objek dengan semua objek lain yang berada pada cluster lain, kemudian ambil nilai paling minimum dengan persamaan : Gambar 2 Rumus Menghitung Rata-Rata Objek Dengan Objek Lain Dalam Cluster Yang Berbeda. Hitung nilai silhouette coefficient dengan persamaan : \u200b Gambar 3 Rumus Menghitung Silhouette Coefficient . Source Code untuk Silhouette for i in range ( 2 , len ( fiturBaru ) - 1 ): kmeans = KMeans ( n_clusters = i , random_state = 0 ) . fit ( fiturBaru ); classnya = kmeans . labels_ s_avg = silhouette_score ( fiturBaru , classnya , random_state = 0 ) print ( Silhouette untuk , i , cluster adalah , s_avg ) print ( kmeans . labels_ ) Metode evaluasi digunakan pada sistem ini adalah metode silhouette coefficient . Metode ini berfungsi untuk menguji kualitas dari cluster yang dihasilkan. Metode ini merupakan metode validasi cluster. 2 Hasil Running : http://nopi-en.blogspot.com/2018/11/pengujian-silhouette-coefficient.html https://lookmylife.wordpress.com/2011/10/03/metode-silhoutte-coeffisien/","title":"Evaluasi"},{"location":"Silhouette/#silhouette","text":"Silhouette merupakan evaluasi cluster apakah cluster yang digunakan sudah baik apa belum. Metode pengujian yang akan digunakan adalah Silhouette Coefficient. Metode silhouette coefficient merupakan gabungan dari dua metode yaitu metode cohesion yang berfungsi untuk mengukur seberapa dekat relasi antara objek dalam sebuah cluster, dan metode separation yang berfungsi untuk mengukur seberapa jauh sebuah cluster terpisah dengan cluster lain. 1 Tahapan perhitungan silhouette coefficient : Hitung rata-rata jarak objek dengan semua objek lain yang berada di dalam satu cluster dengan persamaan : \u200b Gambar 1 Rumus Menghitung Rata-Rata Objek Dengan Objek Lain Dalam Satu Cluster Hitung rata-rata jarak objek dengan semua objek lain yang berada pada cluster lain, kemudian ambil nilai paling minimum dengan persamaan : Gambar 2 Rumus Menghitung Rata-Rata Objek Dengan Objek Lain Dalam Cluster Yang Berbeda. Hitung nilai silhouette coefficient dengan persamaan : \u200b Gambar 3 Rumus Menghitung Silhouette Coefficient .","title":"Silhouette"},{"location":"Silhouette/#source-code-untuk-silhouette","text":"for i in range ( 2 , len ( fiturBaru ) - 1 ): kmeans = KMeans ( n_clusters = i , random_state = 0 ) . fit ( fiturBaru ); classnya = kmeans . labels_ s_avg = silhouette_score ( fiturBaru , classnya , random_state = 0 ) print ( Silhouette untuk , i , cluster adalah , s_avg ) print ( kmeans . labels_ ) Metode evaluasi digunakan pada sistem ini adalah metode silhouette coefficient . Metode ini berfungsi untuk menguji kualitas dari cluster yang dihasilkan. Metode ini merupakan metode validasi cluster. 2","title":"Source Code untuk Silhouette"},{"location":"Silhouette/#hasil-running","text":"http://nopi-en.blogspot.com/2018/11/pengujian-silhouette-coefficient.html https://lookmylife.wordpress.com/2011/10/03/metode-silhoutte-coeffisien/","title":"Hasil Running :"},{"location":"clustering/K-Means/","text":"Clustering Clustering adalah metode penganalisaan data, yang sering dimasukkan sebagai salah satu metode Data Mining, yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu 'wilayah' yang sama dan data dengan karakteristik yang berbeda ke 'wilayah' yang lain. K-Maens K-Maens merupakan salah satu metode yang digunakan untuk melakukan clustering. K-Means Clustering adalah suatu metode penganalisaan data atau metode Data Mining yang melakukan proses pemodelan tanpa supervisi (unsupervised) dan merupakan salah satu metode yang melakukan pengelompokan data dengan sistem partisi. 1 Metode K-Means Clustering berusaha mengelompokkan data yang ada ke dalam beberapa kelompok, dimana data dalam satu kelompok mempunyai karakteristik yang sama satu sama lainnya dan mempunyai karakteristik yang berbeda dengan data yang ada di dalam kelompok yang lain. Metode K-Means Clustering bertujuan untuk meminimalisasikan objective function yang diset dalam proses clustering dengan cara meminimalkan variasi antar data yang ada di dalam suatu cluster dan memaksimalkan variasi dengan data yang ada di cluster lainnya . Data clustering menggunakan metode K-Means Clustering ini secara umum dilakukan dengan algoritma dasar sebagai berikut: Tentukan jumlah cluster Alokasikan data ke dalam cluster secara random Hitung centroid/rata-rata dari data yang ada di masing-masing cluster Alokasikan masing-masing data ke centroid/rata-rata terdekat Kembali ke Step 3, apabila masih ada data yang berpindah cluster atau apabila perubahan nilai centroid, ada yang di atas nilai threshold yang ditentukan atau apabila perubahan nilai pada objective function yang digunakan di atas nilai threshold yang ditentukan Source Code menghitung K-maens dengan penggunakan Libarary kmeans = KMeans ( n_clusters = i , random_state = 0 ) . fit ( fiturBaru ); classnya = kmeans . labels_ Hasil Running : https://informatikalogi.com/algoritma-k-means-clustering/","title":"K-means"},{"location":"clustering/K-Means/#clustering","text":"Clustering adalah metode penganalisaan data, yang sering dimasukkan sebagai salah satu metode Data Mining, yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu 'wilayah' yang sama dan data dengan karakteristik yang berbeda ke 'wilayah' yang lain.","title":"Clustering"},{"location":"clustering/K-Means/#k-maens","text":"K-Maens merupakan salah satu metode yang digunakan untuk melakukan clustering. K-Means Clustering adalah suatu metode penganalisaan data atau metode Data Mining yang melakukan proses pemodelan tanpa supervisi (unsupervised) dan merupakan salah satu metode yang melakukan pengelompokan data dengan sistem partisi. 1 Metode K-Means Clustering berusaha mengelompokkan data yang ada ke dalam beberapa kelompok, dimana data dalam satu kelompok mempunyai karakteristik yang sama satu sama lainnya dan mempunyai karakteristik yang berbeda dengan data yang ada di dalam kelompok yang lain. Metode K-Means Clustering bertujuan untuk meminimalisasikan objective function yang diset dalam proses clustering dengan cara meminimalkan variasi antar data yang ada di dalam suatu cluster dan memaksimalkan variasi dengan data yang ada di cluster lainnya . Data clustering menggunakan metode K-Means Clustering ini secara umum dilakukan dengan algoritma dasar sebagai berikut: Tentukan jumlah cluster Alokasikan data ke dalam cluster secara random Hitung centroid/rata-rata dari data yang ada di masing-masing cluster Alokasikan masing-masing data ke centroid/rata-rata terdekat Kembali ke Step 3, apabila masih ada data yang berpindah cluster atau apabila perubahan nilai centroid, ada yang di atas nilai threshold yang ditentukan atau apabila perubahan nilai pada objective function yang digunakan di atas nilai threshold yang ditentukan","title":"K-Maens"},{"location":"clustering/K-Means/#source-code-menghitung-k-maens-dengan-penggunakan-libarary","text":"kmeans = KMeans ( n_clusters = i , random_state = 0 ) . fit ( fiturBaru ); classnya = kmeans . labels_","title":"Source Code menghitung K-maens dengan penggunakan Libarary"},{"location":"clustering/K-Means/#hasil-running","text":"https://informatikalogi.com/algoritma-k-means-clustering/","title":"Hasil Running :"},{"location":"preprocessing/seleksi_fitur/","text":"Seleksi Fitur Seleksi fitur adalah salah satu tahapan praproses klasifikasi. Seleksi fitur dilakukan dengan cara memilih fitur - fitur yang relevan yang mempengaruhi hasil klasifikasi. Seleksi fitur digunakan untuk mengurangi dimensi data dan fitur - fitur yang tidak relevan. 1 source code untuk seleksi fitur, clustering dan silhouette def pearsonCalculate ( data , u , v ): i, j is an index atas = 0 ; bawah_kiri = 0 ; bawah_kanan = 0 for k in range ( len ( data )): atas += ( data [ k , u ] - meanFitur [ u ]) * ( data [ k , v ] - meanFitur [ v ]) bawah_kiri += ( data [ k , u ] - meanFitur [ u ]) ** 2 bawah_kanan += ( data [ k , v ] - meanFitur [ v ]) ** 2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas / ( bawah_kiri * bawah_kanan ) def meanF ( data ): meanFitur = [] for i in range ( len ( data [ 0 ])): meanFitur . append ( sum ( data [:, i ]) / len ( data )) return np . array ( meanFitur ) def seleksiFiturPearson ( katadasar , data , threshold ): global meanFitur meanFitur = meanF ( data ) u = 0 while u len ( data [ 0 ]): dataBaru = data [:, : u + 1 ] meanBaru = meanFitur [: u + 1 ] katadasarBaru = katadasar [: u + 1 ] v = u while v len ( data [ 0 ]): if u != v : value = pearsonCalculate ( data , u , v ) if value threshold : dataBaru = np . hstack (( dataBaru , data [:, v ] . reshape ( data . shape [ 0 ], 1 ))) meanBaru = np . hstack (( meanBaru , meanFitur [ v ])) katadasarBaru = np . hstack (( katadasarBaru , katadasar [ v ])) v += 1 data = dataBaru meanFitur = meanBaru katadasar = katadasarBaru if u % 50 == 0 : print ( proses : , u , data . shape ) u += 1 return katadasar , data katadasarBaru , fiturBaru = seleksiFiturPearson ( katadasar , tfidf , 0.8 ) for i in range ( 2 , len ( fiturBaru ) - 1 ): kmeans = KMeans ( n_clusters = i , random_state = 0 ) . fit ( fiturBaru ); classnya = kmeans . labels_ s_avg = silhouette_score ( fiturBaru , classnya , random_state = 0 ) print ( Silhouette untuk , i , cluster adalah , s_avg ) print ( kmeans . labels_ ) print ( proses selesai ) with open ( Anggota_cluster.csv , newline = , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) for i in classnya . reshape ( - 1 , 1 ): employee_writer . writerow ( i ) with open ( Seleksi_Fitur.csv , newline = , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) employee_writer . writerow ([ katadasarBaru . tolist ()]) for i in fiturBaru : employee_writer . writerow ( i ) Hasil Running : https://repository.ipb.ac.id/handle/123456789/14020","title":"Seleksi Fitur"},{"location":"preprocessing/seleksi_fitur/#seleksi-fitur","text":"Seleksi fitur adalah salah satu tahapan praproses klasifikasi. Seleksi fitur dilakukan dengan cara memilih fitur - fitur yang relevan yang mempengaruhi hasil klasifikasi. Seleksi fitur digunakan untuk mengurangi dimensi data dan fitur - fitur yang tidak relevan. 1","title":"Seleksi Fitur"},{"location":"preprocessing/seleksi_fitur/#source-code-untuk-seleksi-fitur-clustering-dan-silhouette","text":"def pearsonCalculate ( data , u , v ): i, j is an index atas = 0 ; bawah_kiri = 0 ; bawah_kanan = 0 for k in range ( len ( data )): atas += ( data [ k , u ] - meanFitur [ u ]) * ( data [ k , v ] - meanFitur [ v ]) bawah_kiri += ( data [ k , u ] - meanFitur [ u ]) ** 2 bawah_kanan += ( data [ k , v ] - meanFitur [ v ]) ** 2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas / ( bawah_kiri * bawah_kanan ) def meanF ( data ): meanFitur = [] for i in range ( len ( data [ 0 ])): meanFitur . append ( sum ( data [:, i ]) / len ( data )) return np . array ( meanFitur ) def seleksiFiturPearson ( katadasar , data , threshold ): global meanFitur meanFitur = meanF ( data ) u = 0 while u len ( data [ 0 ]): dataBaru = data [:, : u + 1 ] meanBaru = meanFitur [: u + 1 ] katadasarBaru = katadasar [: u + 1 ] v = u while v len ( data [ 0 ]): if u != v : value = pearsonCalculate ( data , u , v ) if value threshold : dataBaru = np . hstack (( dataBaru , data [:, v ] . reshape ( data . shape [ 0 ], 1 ))) meanBaru = np . hstack (( meanBaru , meanFitur [ v ])) katadasarBaru = np . hstack (( katadasarBaru , katadasar [ v ])) v += 1 data = dataBaru meanFitur = meanBaru katadasar = katadasarBaru if u % 50 == 0 : print ( proses : , u , data . shape ) u += 1 return katadasar , data katadasarBaru , fiturBaru = seleksiFiturPearson ( katadasar , tfidf , 0.8 ) for i in range ( 2 , len ( fiturBaru ) - 1 ): kmeans = KMeans ( n_clusters = i , random_state = 0 ) . fit ( fiturBaru ); classnya = kmeans . labels_ s_avg = silhouette_score ( fiturBaru , classnya , random_state = 0 ) print ( Silhouette untuk , i , cluster adalah , s_avg ) print ( kmeans . labels_ ) print ( proses selesai ) with open ( Anggota_cluster.csv , newline = , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) for i in classnya . reshape ( - 1 , 1 ): employee_writer . writerow ( i ) with open ( Seleksi_Fitur.csv , newline = , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) employee_writer . writerow ([ katadasarBaru . tolist ()]) for i in fiturBaru : employee_writer . writerow ( i )","title":"source code untuk seleksi fitur, clustering dan silhouette"},{"location":"preprocessing/seleksi_fitur/#hasil-running","text":"https://repository.ipb.ac.id/handle/123456789/14020","title":"Hasil Running :"},{"location":"textExtention/TF-IDF/","text":"TF-IDF Tf menyatakan jumlah berapa banyak keberadaan suatu kata dalam satu dokumen. 1 IDF (Inverse Document Frequency) merupakan sebuah perhitungan dari bagaimana kata didistribusikan secara luas pada koleksi dokumen yang bersangkutan. IDF menunjukkan hubungan ketersediaan sebuah kata dalam seluruh dokumen. Semakin sedikit jumlah dokumen yang mengandung kata yang dimaksud, maka nilai IDF semakin besar. 2 Rumus IDF : Rumus TF-IDF : source code tf-idf df = list () for d in range ( len ( matrix [ 0 ])): total = 0 for i in range ( len ( matrix )): if matrix [ i ][ d ] != 0 : total += 1 df . append ( total ) idf = list () for i in df : tmp = 1 + log10 ( len ( matrix ) / ( 1 + i )) idf . append ( tmp ) tf = matrix tfidf = [] for baris in range ( len ( matrix )): tampungBaris = [] for kolom in range ( len ( matrix [ 0 ])): tmp = tf [ baris ][ kolom ] * idf [ kolom ] tampungBaris . append ( tmp ) tfidf . append ( tampungBaris ) tfidf = np . array ( tfidf ) print ( tf_idf ) with open ( tf-idf.csv , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) employee_writer . writerow ( katadasar ) for i in tfidf : employee_writer . writerow ( i ) Hasil Running : https://rahmadya.com/2014/09/25/term-frequency-dan-invers-document-frequency-tf-idf/ https://informatikalogi.com/term-weighting-tf-idf/","title":"TF-IDF"},{"location":"textExtention/TF-IDF/#tf-idf","text":"Tf menyatakan jumlah berapa banyak keberadaan suatu kata dalam satu dokumen. 1 IDF (Inverse Document Frequency) merupakan sebuah perhitungan dari bagaimana kata didistribusikan secara luas pada koleksi dokumen yang bersangkutan. IDF menunjukkan hubungan ketersediaan sebuah kata dalam seluruh dokumen. Semakin sedikit jumlah dokumen yang mengandung kata yang dimaksud, maka nilai IDF semakin besar. 2 Rumus IDF : Rumus TF-IDF :","title":"TF-IDF"},{"location":"textExtention/TF-IDF/#source-code-tf-idf","text":"df = list () for d in range ( len ( matrix [ 0 ])): total = 0 for i in range ( len ( matrix )): if matrix [ i ][ d ] != 0 : total += 1 df . append ( total ) idf = list () for i in df : tmp = 1 + log10 ( len ( matrix ) / ( 1 + i )) idf . append ( tmp ) tf = matrix tfidf = [] for baris in range ( len ( matrix )): tampungBaris = [] for kolom in range ( len ( matrix [ 0 ])): tmp = tf [ baris ][ kolom ] * idf [ kolom ] tampungBaris . append ( tmp ) tfidf . append ( tampungBaris ) tfidf = np . array ( tfidf ) print ( tf_idf ) with open ( tf-idf.csv , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) employee_writer . writerow ( katadasar ) for i in tfidf : employee_writer . writerow ( i )","title":"source code tf-idf"},{"location":"textExtention/TF-IDF/#hasil-running","text":"https://rahmadya.com/2014/09/25/term-frequency-dan-invers-document-frequency-tf-idf/ https://informatikalogi.com/term-weighting-tf-idf/","title":"Hasil Running :"},{"location":"textExtention/VSM/","text":"TextProcessing Pertama Tokenisasi adalah proses untuk membagi teks yang dapat berupa kalimat, paragraf atau dokumen, menjadi token - token / bagian - bagian tertentu. Sebagai contoh, tokenisasi dari kalimat \"Aku baru saja makan bakso pedas\" menghasilkan enam token, yakni: \"Aku\", \"baru\", \"saja\", \"makan\", \"bakso\", \"pedas\". Biasanya, yang menjadi acuan pemisah antar token adalah spasi dan tanda baca. Tokenisasi seringkali dipakai dalam ilmu linguistik dan hasil tokenisasi berguna untuk analisis teks lebih lanjut. 1 Kedua Stop words adalah kata umum (common words) yang biasanya muncul dalam jumlah besar dan dianggap tidak memiliki makna. Stop words umumnya dimanfaatkan dalam task information retrieval. Contoh stop words untuk bahasa Inggris diantaranya \u201cof\u201d, \u201cthe\u201d. Sedangkan untuk bahasa Indonesia diantaranya \u201cyang\u201d, \u201cdi\u201d, \u201cke\u201d. 1 Ketiga Stemmming merupakan salah satu proses dari pembuatan sistem temu kembali, dimana proses stemming akan dilakukan setelah proses filtering. Proses stemming ini membuat term yang ada pada tabel filtering menjadi kata dasar, dengan menghilankan semua imbuhan yang ada pada kata tersebut ( imbuhan meng-, me-, kan-, di- , i, pe, peng-, a-, dll.). 1 Pentingnya stemming dalam proses pembuatan sistem temu kembali yakni dimana saat menghilangkan imbuhan pada sebuah kata menjadi hal yang perlu diperhatikan. Karena dalam proses stemming yang penting yakni terlebih untuk menghilangkan imbuhan pada awalan setelah itu akhiran. Apabila yang dilakukan adalah sebaliknya maka tidak akan ditemukan kata dasar yang tepat dan sesuai dengan kamus bahasa. Dimana dari hasil proses tersebut akan didapatkan sebuah informasi mengenai banyaknya term yang muncul dalam sebuah dokumen setelah dilakukan perhitungan term frequency. 1 Source Code Text Processing factory = StopWordRemoverFactory () stopword = factory . create_stop_word_remover () factory = StemmerFactory () stemmer = factory . create_stemmer () tmp = for i in isi : tmp = tmp + + i hasil = [] for i in tmp . split (): try : if i . isalpha () and ( not i in hasil ) and len ( i ) 1 : # Menghilangkan Kata tidak penting stop = stopword . remove ( i ) if stop != : stem = stemmer . stem ( stop ) hasil . append ( stem ) except : continue katadasar = np . array ( hasil ) source code untuk menyeleksi kata dasar agar sesuai dengan KBBI #KBBI koneksi = sqlite3 . connect ( KBI.db ) cur_kbi = koneksi . execute ( SELECT* from KATA ) def LinearSearch ( kbi , kata ): found = False posisi = 0 while posisi len ( kata ) and not found : if kata [ posisi ] == kbi : found = True posisi = posisi + 1 return found berhasil = [] for kata in cur_kbi : ketemu = LinearSearch ( kata [ 0 ], katadasar ) if ketemu : kata = kata [ 0 ] berhasil . append ( kata ) print ( berhasil ) katadasar = np . array ( berhasil ) VSM Vector Space Model (VSM) digunakan sebagai representasi dari kumpulan dataset dokumen teks. Dokumen dalam Vector Space Model (VSM) berupa matriks yang berisi bobot seluruh kata pada tiap dokumen. Bobot tersebut menyatakan kepentingan atau kontribusi kata terhadap suatu dokumen dan kumpulan dokumen. 2 source code untuk VSM matrix = [] for row in isi : tamp_isi = [] for a in katadasar : tamp_isi . append ( row . lower () . count ( a )) matrix . append ( tamp_isi ) print ( vsm ) with open ( data_matrix.csv , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) employee_writer . writerow ( katadasar ) for i in matrix : employee_writer . writerow ( i ) kata yang sudah terseleksi kemudian di simpan ke dalam csv dalam bentuk matrix. Hasil Running : http://pentingnyakesehatananda.blogspot.com/2017/09/pengertian-tokenisasi-stopword-removal.html https://informatikalogi.com/vector-space-model-pengukuran-jarak/","title":"VSM"},{"location":"textExtention/VSM/#textprocessing","text":"Pertama Tokenisasi adalah proses untuk membagi teks yang dapat berupa kalimat, paragraf atau dokumen, menjadi token - token / bagian - bagian tertentu. Sebagai contoh, tokenisasi dari kalimat \"Aku baru saja makan bakso pedas\" menghasilkan enam token, yakni: \"Aku\", \"baru\", \"saja\", \"makan\", \"bakso\", \"pedas\". Biasanya, yang menjadi acuan pemisah antar token adalah spasi dan tanda baca. Tokenisasi seringkali dipakai dalam ilmu linguistik dan hasil tokenisasi berguna untuk analisis teks lebih lanjut. 1 Kedua Stop words adalah kata umum (common words) yang biasanya muncul dalam jumlah besar dan dianggap tidak memiliki makna. Stop words umumnya dimanfaatkan dalam task information retrieval. Contoh stop words untuk bahasa Inggris diantaranya \u201cof\u201d, \u201cthe\u201d. Sedangkan untuk bahasa Indonesia diantaranya \u201cyang\u201d, \u201cdi\u201d, \u201cke\u201d. 1 Ketiga Stemmming merupakan salah satu proses dari pembuatan sistem temu kembali, dimana proses stemming akan dilakukan setelah proses filtering. Proses stemming ini membuat term yang ada pada tabel filtering menjadi kata dasar, dengan menghilankan semua imbuhan yang ada pada kata tersebut ( imbuhan meng-, me-, kan-, di- , i, pe, peng-, a-, dll.). 1 Pentingnya stemming dalam proses pembuatan sistem temu kembali yakni dimana saat menghilangkan imbuhan pada sebuah kata menjadi hal yang perlu diperhatikan. Karena dalam proses stemming yang penting yakni terlebih untuk menghilangkan imbuhan pada awalan setelah itu akhiran. Apabila yang dilakukan adalah sebaliknya maka tidak akan ditemukan kata dasar yang tepat dan sesuai dengan kamus bahasa. Dimana dari hasil proses tersebut akan didapatkan sebuah informasi mengenai banyaknya term yang muncul dalam sebuah dokumen setelah dilakukan perhitungan term frequency. 1","title":"TextProcessing"},{"location":"textExtention/VSM/#source-code-text-processing","text":"factory = StopWordRemoverFactory () stopword = factory . create_stop_word_remover () factory = StemmerFactory () stemmer = factory . create_stemmer () tmp = for i in isi : tmp = tmp + + i hasil = [] for i in tmp . split (): try : if i . isalpha () and ( not i in hasil ) and len ( i ) 1 : # Menghilangkan Kata tidak penting stop = stopword . remove ( i ) if stop != : stem = stemmer . stem ( stop ) hasil . append ( stem ) except : continue katadasar = np . array ( hasil )","title":"Source Code Text Processing"},{"location":"textExtention/VSM/#source-code-untuk-menyeleksi-kata-dasar-agar-sesuai-dengan-kbbi","text":"#KBBI koneksi = sqlite3 . connect ( KBI.db ) cur_kbi = koneksi . execute ( SELECT* from KATA ) def LinearSearch ( kbi , kata ): found = False posisi = 0 while posisi len ( kata ) and not found : if kata [ posisi ] == kbi : found = True posisi = posisi + 1 return found berhasil = [] for kata in cur_kbi : ketemu = LinearSearch ( kata [ 0 ], katadasar ) if ketemu : kata = kata [ 0 ] berhasil . append ( kata ) print ( berhasil ) katadasar = np . array ( berhasil )","title":"source code untuk menyeleksi kata dasar agar sesuai dengan KBBI"},{"location":"textExtention/VSM/#vsm","text":"Vector Space Model (VSM) digunakan sebagai representasi dari kumpulan dataset dokumen teks. Dokumen dalam Vector Space Model (VSM) berupa matriks yang berisi bobot seluruh kata pada tiap dokumen. Bobot tersebut menyatakan kepentingan atau kontribusi kata terhadap suatu dokumen dan kumpulan dokumen. 2","title":"VSM"},{"location":"textExtention/VSM/#source-code-untuk-vsm","text":"matrix = [] for row in isi : tamp_isi = [] for a in katadasar : tamp_isi . append ( row . lower () . count ( a )) matrix . append ( tamp_isi ) print ( vsm ) with open ( data_matrix.csv , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) employee_writer . writerow ( katadasar ) for i in matrix : employee_writer . writerow ( i ) kata yang sudah terseleksi kemudian di simpan ke dalam csv dalam bentuk matrix.","title":"source code untuk VSM"},{"location":"textExtention/VSM/#hasil-running","text":"http://pentingnyakesehatananda.blogspot.com/2017/09/pengertian-tokenisasi-stopword-removal.html https://informatikalogi.com/vector-space-model-pengukuran-jarak/","title":"Hasil Running :"}]}