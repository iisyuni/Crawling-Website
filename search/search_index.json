{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Web Crawler Menggunakan Python Nama : Iis Yuni Harianti NIM : 160411100086 Semester : 6 Mata kuliah : Pengembangan dan Pencarian Web Dosen Pengampu : Mulaab, S.Si., M.Kom Library yang di butuhkan untuk mengcrawl untuk memulai melakukan crawl pastikan python yang digunakan sudah memiliki library sebagai berikut : BeautifulSoup4 digunakan untuk mengambil data berdasarkan tag html. requests digunakan untuk menganbil satu halaman html. Sastrawi digunakan untuk mengubah kata menjadi kata dasar. numpy digunakan untuk mengolah data numerik diolah menjadi matrik scikit-learn digunakan untuk menyediakan rumus untuk menghitung K-means, Silhouette. from math import log10 import requests from bs4 import BeautifulSoup import sqlite3 from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from Sastrawi.Stemmer.StemmerFactory import StemmerFactory import csv from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score import numpy as np import warnings","title":"Pengantar"},{"location":"#web-crawler-menggunakan-python","text":"Nama : Iis Yuni Harianti NIM : 160411100086 Semester : 6 Mata kuliah : Pengembangan dan Pencarian Web Dosen Pengampu : Mulaab, S.Si., M.Kom","title":"Web Crawler Menggunakan Python"},{"location":"#library-yang-di-butuhkan-untuk-mengcrawl","text":"untuk memulai melakukan crawl pastikan python yang digunakan sudah memiliki library sebagai berikut : BeautifulSoup4 digunakan untuk mengambil data berdasarkan tag html. requests digunakan untuk menganbil satu halaman html. Sastrawi digunakan untuk mengubah kata menjadi kata dasar. numpy digunakan untuk mengolah data numerik diolah menjadi matrik scikit-learn digunakan untuk menyediakan rumus untuk menghitung K-means, Silhouette. from math import log10 import requests from bs4 import BeautifulSoup import sqlite3 from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from Sastrawi.Stemmer.StemmerFactory import StemmerFactory import csv from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score import numpy as np import warnings","title":"Library yang di  butuhkan untuk mengcrawl"},{"location":"Crawl_link/","text":"Web Structure Mining Web structure mining dikenal juga sebagai web log mining adalah teknik yang digunakan untuk menemukan struktur link dari hyperlink dan membangun rangkuman website dan halaman web. Salah satu manfaatnya adlah untuk menentukan pagerank pada suatu halaman web. 1 Crawl Link Yang akan di lakukan kali ini yaitu mengcrawl sebuah link dari sebuah wesite. web target : https://www.kompasiana.com/ library yang digunakan import pandas as pd import requests from bs4 import BeautifulSoup source code untuk mengcrawl link def simplifiedURL ( url ): if www. in url : ind = url . index ( www. ) + 4 url = http:// + url [ ind :] if url [ - 1 ] == / : url = url [: - 1 ] parts = url . split ( / ) url = for i in range ( 3 ): url += parts [ i ] + / return url Code di atas digunakan untuk menyamakan format link url yang sudah di crawl. def crawl ( url , max_deep , show = False , deep = 0 , done = []): global edgelist deep += 1 url = simplifiedURL ( url ) if not url in done : links = getAllLinks ( url ) done . append ( url ) if show : if deep == 1 : print ( url ) else : print ( | , end = ) for i in range ( deep - 1 ): print ( -- , end = ) print ( ( %d ) %s % ( len ( links ), url )) for link in links : link = simplifiedURL ( link ) edge = ( url , link ) if not edge in edgelist : edgelist . append ( edge ) if ( deep != max_deep ): crawl ( link , max_deep , show , deep , done ) Code fungsi di atas di gunakan untuk melakukan pengecekan apakah link yang di crawl ada yang sama atau tidak. serta code di atas juga melakukan crawling pada website dengan menentukan kedalaman ketika melakukan crawl. Dengan parameter fungsi yaitu (url, max_deep, show=False, deep=0, done=[]), url adalah alamat website yang menjadi target crawl, max_deep yaitu digunakan untuk menetukan batasan kedalaman saat melakukan crawl, show untuk menampilkan proses atau tidak, deep untuk mengecek kedalaman yang di crawl. done untuk mengecek url yang sudah pernah di crawl atau belum jika belum pernah akan di masukkan kedalam list. def getAllLinks ( src ): try : page = requests . get ( src ) soup = BeautifulSoup ( page . content , html.parser ) tags = soup . findAll ( a ) links = [] for tag in tags : try : link = tag [ href ] if not link in links and http in link : links . append ( link ) except KeyError : pass return links except : return list () root = https://www.kompasiana.com/ nodelist = [ root ] edgelist = [] #crawl crawl ( root , 3 , show = True ) edgelistFrame = pd . DataFrame ( edgelist , None , ( From , To )) Code di atas di gunakan untuk meng crawl semua link yang ada pada web site https://www.kompasiana.com/ serta menjalankan berbagai fungsi yang ada. Hasil Running : Referensi : https://sis.binus.ac.id/2016/12/15/teori-text-mining-dan-web-mining/","title":"Crawl link"},{"location":"Crawl_link/#web-structure-mining","text":"Web structure mining dikenal juga sebagai web log mining adalah teknik yang digunakan untuk menemukan struktur link dari hyperlink dan membangun rangkuman website dan halaman web. Salah satu manfaatnya adlah untuk menentukan pagerank pada suatu halaman web. 1","title":"Web Structure Mining"},{"location":"Crawl_link/#crawl-link","text":"Yang akan di lakukan kali ini yaitu mengcrawl sebuah link dari sebuah wesite. web target : https://www.kompasiana.com/","title":"Crawl Link"},{"location":"Crawl_link/#library-yang-digunakan","text":"import pandas as pd import requests from bs4 import BeautifulSoup","title":"library yang digunakan"},{"location":"Crawl_link/#source-code-untuk-mengcrawl-link","text":"def simplifiedURL ( url ): if www. in url : ind = url . index ( www. ) + 4 url = http:// + url [ ind :] if url [ - 1 ] == / : url = url [: - 1 ] parts = url . split ( / ) url = for i in range ( 3 ): url += parts [ i ] + / return url Code di atas digunakan untuk menyamakan format link url yang sudah di crawl. def crawl ( url , max_deep , show = False , deep = 0 , done = []): global edgelist deep += 1 url = simplifiedURL ( url ) if not url in done : links = getAllLinks ( url ) done . append ( url ) if show : if deep == 1 : print ( url ) else : print ( | , end = ) for i in range ( deep - 1 ): print ( -- , end = ) print ( ( %d ) %s % ( len ( links ), url )) for link in links : link = simplifiedURL ( link ) edge = ( url , link ) if not edge in edgelist : edgelist . append ( edge ) if ( deep != max_deep ): crawl ( link , max_deep , show , deep , done ) Code fungsi di atas di gunakan untuk melakukan pengecekan apakah link yang di crawl ada yang sama atau tidak. serta code di atas juga melakukan crawling pada website dengan menentukan kedalaman ketika melakukan crawl. Dengan parameter fungsi yaitu (url, max_deep, show=False, deep=0, done=[]), url adalah alamat website yang menjadi target crawl, max_deep yaitu digunakan untuk menetukan batasan kedalaman saat melakukan crawl, show untuk menampilkan proses atau tidak, deep untuk mengecek kedalaman yang di crawl. done untuk mengecek url yang sudah pernah di crawl atau belum jika belum pernah akan di masukkan kedalam list. def getAllLinks ( src ): try : page = requests . get ( src ) soup = BeautifulSoup ( page . content , html.parser ) tags = soup . findAll ( a ) links = [] for tag in tags : try : link = tag [ href ] if not link in links and http in link : links . append ( link ) except KeyError : pass return links except : return list () root = https://www.kompasiana.com/ nodelist = [ root ] edgelist = [] #crawl crawl ( root , 3 , show = True ) edgelistFrame = pd . DataFrame ( edgelist , None , ( From , To )) Code di atas di gunakan untuk meng crawl semua link yang ada pada web site https://www.kompasiana.com/ serta menjalankan berbagai fungsi yang ada.","title":"source code untuk mengcrawl link"},{"location":"Crawl_link/#hasil-running","text":"Referensi : https://sis.binus.ac.id/2016/12/15/teori-text-mining-dan-web-mining/","title":"Hasil Running :"},{"location":"Crawling/","text":"Crawling Web Crawler digunakan untuk mengambil data berupa teks, audio, foto bahkan video pada sebuah website. source code untuk mengcrawl data Proses crawling dalam suatu website dimulai dari mendata seluruh url dari website, menelusurinya satu-persatu, kemudian memasukkannya dalam daftar halaman pada indeks search engine, sehingga setiap kali ada perubahan pada website, akan terupdate secara otomatis 1 . src = https://www.kompasiana.com/olahraga/ page = requests . get ( src ) soup = BeautifulSoup ( page . content , html.parser ) artikel = soup . findAll ( class_ = title mt40 ) koneksi = sqlite3 . connect ( db_data.db ) koneksi . execute ( CREATE TABLE if not exists kompasiana (judul TEXT NOT NULL, isi TEXT NOT NULL); ) for i in range ( len ( artikel )): link = artikel [ i ] . find ( a )[ href ] page = requests . get ( link ) soup = BeautifulSoup ( page . content , html.parser ) judul = soup . find ( class_ = title ) . getText () isi = soup . find ( class_ = read-content col-lg-9 col-md-9 col-sm-9 col-xs-9 ) paragraf = isi . findAll ( p ) p = for s in paragraf : p += s . getText () + cek = koneksi . execute ( SELECT * FROM kompasiana where judul=? , ( judul ,)) cek = cek . fetchall () if len ( cek ) == 0 : koneksi . execute ( INSERT INTO kompasiana values (?,?) , ( judul , p )); koneksi . commit () tampil = koneksi . execute ( SELECT * FROM kompasiana ) with open ( data_crawler.csv , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) for i in tampil : employee_writer . writerow ( i ) tampil = koneksi . execute ( SELECT * FROM kompasiana ) isi = [] for row in tampil : isi . append ( row [ 1 ]) #print(row) print ( crawl ) Data yang telah di crawl kemudian di simpan ke dalam database yaitu SQlite Hasil Running : Referensi : http://www.teknologi-bigdata.com/2016/07/web-crawling-di-era-big-data.html","title":"Crawling"},{"location":"Crawling/#crawling","text":"Web Crawler digunakan untuk mengambil data berupa teks, audio, foto bahkan video pada sebuah website.","title":"Crawling"},{"location":"Crawling/#source-code-untuk-mengcrawl-data","text":"Proses crawling dalam suatu website dimulai dari mendata seluruh url dari website, menelusurinya satu-persatu, kemudian memasukkannya dalam daftar halaman pada indeks search engine, sehingga setiap kali ada perubahan pada website, akan terupdate secara otomatis 1 . src = https://www.kompasiana.com/olahraga/ page = requests . get ( src ) soup = BeautifulSoup ( page . content , html.parser ) artikel = soup . findAll ( class_ = title mt40 ) koneksi = sqlite3 . connect ( db_data.db ) koneksi . execute ( CREATE TABLE if not exists kompasiana (judul TEXT NOT NULL, isi TEXT NOT NULL); ) for i in range ( len ( artikel )): link = artikel [ i ] . find ( a )[ href ] page = requests . get ( link ) soup = BeautifulSoup ( page . content , html.parser ) judul = soup . find ( class_ = title ) . getText () isi = soup . find ( class_ = read-content col-lg-9 col-md-9 col-sm-9 col-xs-9 ) paragraf = isi . findAll ( p ) p = for s in paragraf : p += s . getText () + cek = koneksi . execute ( SELECT * FROM kompasiana where judul=? , ( judul ,)) cek = cek . fetchall () if len ( cek ) == 0 : koneksi . execute ( INSERT INTO kompasiana values (?,?) , ( judul , p )); koneksi . commit () tampil = koneksi . execute ( SELECT * FROM kompasiana ) with open ( data_crawler.csv , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) for i in tampil : employee_writer . writerow ( i ) tampil = koneksi . execute ( SELECT * FROM kompasiana ) isi = [] for row in tampil : isi . append ( row [ 1 ]) #print(row) print ( crawl ) Data yang telah di crawl kemudian di simpan ke dalam database yaitu SQlite","title":"source code untuk mengcrawl data"},{"location":"Crawling/#hasil-running","text":"Referensi : http://www.teknologi-bigdata.com/2016/07/web-crawling-di-era-big-data.html","title":"Hasil Running :"},{"location":"Graph/","text":"Graph Graph adalah kumpulan dati titik (node) dan garis dimana pasangan \u2013 pasangan titik (node) tersebut dihubungkan oleh segmen garis. Node ini biasa disebut simpul (vertex) dan segmen garis disebut ruas (edge) Simpul dan ruas dalam graph dapat diperluas dengan penambahan informasi. Sebagai contoh, simpul bias diberi nomor atau label dan ruas dapat diberi nilai juga. Perluasan dengan pemberian informasi ini sangat berguna dalam penggunaan graph untuk banyak aplikasi computer. Contoh, graph dengan simpul yang merepresentasikan kota dan ruas merepresentasikan jarak yang ditempuh diantara kota \u2013 kota tersebut. (atau harga tiket pesawat antara kota \u2013 kota tersebut). 1 Library yang digunakan import networkx as nx import matplotlib.pyplot as plt source code untuk membuat Graph g = nx . from_pandas_edgelist ( edgelistFrame , From , To , None , nx . DiGraph ()) pos = nx . spring_layout ( g ) nodelist = g . nodes label = {} for i , key in enumerate ( nodelist ): label [ key ] = i nx . draw ( g , pos ) nx . draw_networkx_labels ( g , pos , label , font_color = w ) plt . axis ( off ) plt . show () code membuat graph dari link yang sudah di crawl yang saling berhubungan dengan link lainnya. dan menghasilkan bentuk grap seperti gambar berikut ini. Hasil Running : Referensi : https://strukturdata01.wordpress.com/2015/01/26/graph/","title":"Graph"},{"location":"Graph/#graph","text":"Graph adalah kumpulan dati titik (node) dan garis dimana pasangan \u2013 pasangan titik (node) tersebut dihubungkan oleh segmen garis. Node ini biasa disebut simpul (vertex) dan segmen garis disebut ruas (edge) Simpul dan ruas dalam graph dapat diperluas dengan penambahan informasi. Sebagai contoh, simpul bias diberi nomor atau label dan ruas dapat diberi nilai juga. Perluasan dengan pemberian informasi ini sangat berguna dalam penggunaan graph untuk banyak aplikasi computer. Contoh, graph dengan simpul yang merepresentasikan kota dan ruas merepresentasikan jarak yang ditempuh diantara kota \u2013 kota tersebut. (atau harga tiket pesawat antara kota \u2013 kota tersebut). 1","title":"Graph"},{"location":"Graph/#library-yang-digunakan","text":"import networkx as nx import matplotlib.pyplot as plt","title":"Library yang digunakan"},{"location":"Graph/#source-code-untuk-membuat-graph","text":"g = nx . from_pandas_edgelist ( edgelistFrame , From , To , None , nx . DiGraph ()) pos = nx . spring_layout ( g ) nodelist = g . nodes label = {} for i , key in enumerate ( nodelist ): label [ key ] = i nx . draw ( g , pos ) nx . draw_networkx_labels ( g , pos , label , font_color = w ) plt . axis ( off ) plt . show () code membuat graph dari link yang sudah di crawl yang saling berhubungan dengan link lainnya. dan menghasilkan bentuk grap seperti gambar berikut ini.","title":"source code untuk membuat Graph"},{"location":"Graph/#hasil-running","text":"Referensi : https://strukturdata01.wordpress.com/2015/01/26/graph/","title":"Hasil Running :"},{"location":"Note/","text":"Note Untuk menjalankan programnya pastikan terhubung dengan koneksi internet. Agar tidak terjadi error. Untuk database KBBI (KBI.db) pastikan tersimpan dalam satu folder dengan file programnya crawling.py Untuk melihat program lebih lengkapnya di Web Crawling","title":"Note"},{"location":"Note/#note","text":"Untuk menjalankan programnya pastikan terhubung dengan koneksi internet. Agar tidak terjadi error. Untuk database KBBI (KBI.db) pastikan tersimpan dalam satu folder dengan file programnya crawling.py Untuk melihat program lebih lengkapnya di Web Crawling","title":"Note"},{"location":"Note_web_structure/","text":"Note Untuk menjalankan programnya pastikan terhubung dengan koneksi internet. Agar tidak terjadi error. Untuk melihat program lebih lengkapnya di Web Structure Mining","title":"Note"},{"location":"Note_web_structure/#note","text":"Untuk menjalankan programnya pastikan terhubung dengan koneksi internet. Agar tidak terjadi error. Untuk melihat program lebih lengkapnya di Web Structure Mining","title":"Note"},{"location":"PageRank/","text":"PageRank PageRank adalah sebuah algoritma yang telah dipatenkan yang berfungsi menentukan situs web mana yang lebih penting/populer. PageRank merupakan salah satu fitur utama mesin pencari Google dan diciptakan oleh pendirinya, Larry Page dan Sergey Brin yang merupakan mahasiswa Ph.D. Universitas Stanford. 1 Google pagerank memperggunakan sebuah sistem yang baku untuk menilai bobot sebuah halaman web/blog. Pada prinsipnya sebuah halaman dari blog yang memiliki sejumlah link masuk maka akan diindeks sebagai halaman penting, sehingga google memberikan status pagerank tertentu. Google melakukan perhitungan dan analisis terhadap halaman yang mendapat link (vote). Vote yang didapatkan dari halaman lain yang penting maka akan membantu meningkatkan kualitas pagerank halaman tersebut. 2 PR = 0.15 + 0.85 (PR(T1)/C(T1) + PR(T2)/C(T2) + ... + PR(Tn)/C(Tn)) Keterangan: PR(T1) itu nilai pagerank halaman T1 C(T1) itu jumlah link yang keluar dari halaman T1 berlaku seterusnya dari T2 sampai Tn 3 Contoh penghitungan pagerank manual dengan ketentuan 3 iterasi : Sistem back link Ada 3 buah halaman web A, B dan C dimana halaman web A mempunyai link ke halaman Web B dan C, halaman web B mempunyai link ke halaman web C sedangkan halaman web C mempunyai link ke halaman web A. Berapakah PageRank dari masing-masing halaman web. 4 Libarary yang digunakan import networkx as nx source code untuk menghitung Pagerank damping = 0.85 max_iterr = 100 error_toleransi = 0.0001 pr = nx . pagerank ( g , alpha = damping , max_iter = max_iterr , tol = error_toleransi ) Code di atas digunakan untuk menghitung pagerank dengan ketentuan damping 0.85, dengan batasan hingga 100 iterasi dan 0.0001 error toleransi. urut = data . copy () for x in range ( len ( urut )): for y in range ( len ( urut )): if urut [ x ][ 0 ] urut [ y ][ 0 ]: urut [ x ], urut [ y ] = urut [ y ], urut [ x ] urut = pd . DataFrame ( urut , None , ( PageRank , Node )) print ( urut ) Code di atas digunakan untuk menampilkan pagerank secara berurutan dari yang terbesar hingga yang terkecil. Jika pagerank suatu link semakin besar maka link tersebut merupakan website penting dan populer. Seperti pada gambar berikut. Hasil Running : Referensi : https://id.wikipedia.org/wiki/PageRank http://caratrikterbaru.blogspot.com/2011/07/rumus-perhitungan-google-pagerank.html http://trikmudahseo.blogspot.com/2012/11/apa-itu-pagerank-dan-cara-menghitung.html http://gembong.lecture.ub.ac.id/seri-tutorial-pagerank-kalkulasi-pagerank/","title":"PageRank"},{"location":"PageRank/#pagerank","text":"PageRank adalah sebuah algoritma yang telah dipatenkan yang berfungsi menentukan situs web mana yang lebih penting/populer. PageRank merupakan salah satu fitur utama mesin pencari Google dan diciptakan oleh pendirinya, Larry Page dan Sergey Brin yang merupakan mahasiswa Ph.D. Universitas Stanford. 1 Google pagerank memperggunakan sebuah sistem yang baku untuk menilai bobot sebuah halaman web/blog. Pada prinsipnya sebuah halaman dari blog yang memiliki sejumlah link masuk maka akan diindeks sebagai halaman penting, sehingga google memberikan status pagerank tertentu. Google melakukan perhitungan dan analisis terhadap halaman yang mendapat link (vote). Vote yang didapatkan dari halaman lain yang penting maka akan membantu meningkatkan kualitas pagerank halaman tersebut. 2 PR = 0.15 + 0.85 (PR(T1)/C(T1) + PR(T2)/C(T2) + ... + PR(Tn)/C(Tn)) Keterangan: PR(T1) itu nilai pagerank halaman T1 C(T1) itu jumlah link yang keluar dari halaman T1 berlaku seterusnya dari T2 sampai Tn 3 Contoh penghitungan pagerank manual dengan ketentuan 3 iterasi : Sistem back link Ada 3 buah halaman web A, B dan C dimana halaman web A mempunyai link ke halaman Web B dan C, halaman web B mempunyai link ke halaman web C sedangkan halaman web C mempunyai link ke halaman web A. Berapakah PageRank dari masing-masing halaman web. 4","title":"PageRank"},{"location":"PageRank/#libarary-yang-digunakan","text":"import networkx as nx","title":"Libarary yang digunakan"},{"location":"PageRank/#source-code-untuk-menghitung-pagerank","text":"damping = 0.85 max_iterr = 100 error_toleransi = 0.0001 pr = nx . pagerank ( g , alpha = damping , max_iter = max_iterr , tol = error_toleransi ) Code di atas digunakan untuk menghitung pagerank dengan ketentuan damping 0.85, dengan batasan hingga 100 iterasi dan 0.0001 error toleransi. urut = data . copy () for x in range ( len ( urut )): for y in range ( len ( urut )): if urut [ x ][ 0 ] urut [ y ][ 0 ]: urut [ x ], urut [ y ] = urut [ y ], urut [ x ] urut = pd . DataFrame ( urut , None , ( PageRank , Node )) print ( urut ) Code di atas digunakan untuk menampilkan pagerank secara berurutan dari yang terbesar hingga yang terkecil. Jika pagerank suatu link semakin besar maka link tersebut merupakan website penting dan populer. Seperti pada gambar berikut.","title":"source code untuk menghitung Pagerank"},{"location":"PageRank/#hasil-running","text":"Referensi : https://id.wikipedia.org/wiki/PageRank http://caratrikterbaru.blogspot.com/2011/07/rumus-perhitungan-google-pagerank.html http://trikmudahseo.blogspot.com/2012/11/apa-itu-pagerank-dan-cara-menghitung.html http://gembong.lecture.ub.ac.id/seri-tutorial-pagerank-kalkulasi-pagerank/","title":"Hasil Running :"},{"location":"Silhouette/","text":"Silhouette Silhouette merupakan evaluasi cluster apakah cluster yang digunakan sudah baik apa belum. Metode pengujian yang akan digunakan adalah Silhouette Coefficient. Metode silhouette coefficient merupakan gabungan dari dua metode yaitu metode cohesion yang berfungsi untuk mengukur seberapa dekat relasi antara objek dalam sebuah cluster, dan metode separation yang berfungsi untuk mengukur seberapa jauh sebuah cluster terpisah dengan cluster lain. 1 Tahapan perhitungan silhouette coefficient : Hitung rata-rata jarak objek dengan semua objek lain yang berada di dalam satu cluster dengan persamaan : \u200b Gambar 1 Rumus Menghitung Rata-Rata Objek Dengan Objek Lain Dalam Satu Cluster Hitung rata-rata jarak objek dengan semua objek lain yang berada pada cluster lain, kemudian ambil nilai paling minimum dengan persamaan : Gambar 2 Rumus Menghitung Rata-Rata Objek Dengan Objek Lain Dalam Cluster Yang Berbeda. Hitung nilai silhouette coefficient dengan persamaan : \u200b Gambar 3 Rumus Menghitung Silhouette Coefficient . Source Code untuk Silhouette for i in range ( 2 , len ( fiturBaru ) - 1 ): kmeans = KMeans ( n_clusters = i , random_state = 0 ) . fit ( fiturBaru ); classnya = kmeans . labels_ s_avg = silhouette_score ( fiturBaru , classnya , random_state = 0 ) print ( Silhouette untuk , i , cluster adalah , s_avg ) print ( kmeans . labels_ ) Metode evaluasi digunakan pada sistem ini adalah metode silhouette coefficient . Metode ini berfungsi untuk menguji kualitas dari cluster yang dihasilkan. Metode ini merupakan metode validasi cluster. 2 Hasil Running : Referensi : http://nopi-en.blogspot.com/2018/11/pengujian-silhouette-coefficient.html https://lookmylife.wordpress.com/2011/10/03/metode-silhoutte-coeffisien/","title":"Evaluasi"},{"location":"Silhouette/#silhouette","text":"Silhouette merupakan evaluasi cluster apakah cluster yang digunakan sudah baik apa belum. Metode pengujian yang akan digunakan adalah Silhouette Coefficient. Metode silhouette coefficient merupakan gabungan dari dua metode yaitu metode cohesion yang berfungsi untuk mengukur seberapa dekat relasi antara objek dalam sebuah cluster, dan metode separation yang berfungsi untuk mengukur seberapa jauh sebuah cluster terpisah dengan cluster lain. 1 Tahapan perhitungan silhouette coefficient : Hitung rata-rata jarak objek dengan semua objek lain yang berada di dalam satu cluster dengan persamaan : \u200b Gambar 1 Rumus Menghitung Rata-Rata Objek Dengan Objek Lain Dalam Satu Cluster Hitung rata-rata jarak objek dengan semua objek lain yang berada pada cluster lain, kemudian ambil nilai paling minimum dengan persamaan : Gambar 2 Rumus Menghitung Rata-Rata Objek Dengan Objek Lain Dalam Cluster Yang Berbeda. Hitung nilai silhouette coefficient dengan persamaan : \u200b Gambar 3 Rumus Menghitung Silhouette Coefficient .","title":"Silhouette"},{"location":"Silhouette/#source-code-untuk-silhouette","text":"for i in range ( 2 , len ( fiturBaru ) - 1 ): kmeans = KMeans ( n_clusters = i , random_state = 0 ) . fit ( fiturBaru ); classnya = kmeans . labels_ s_avg = silhouette_score ( fiturBaru , classnya , random_state = 0 ) print ( Silhouette untuk , i , cluster adalah , s_avg ) print ( kmeans . labels_ ) Metode evaluasi digunakan pada sistem ini adalah metode silhouette coefficient . Metode ini berfungsi untuk menguji kualitas dari cluster yang dihasilkan. Metode ini merupakan metode validasi cluster. 2","title":"Source Code untuk Silhouette"},{"location":"Silhouette/#hasil-running","text":"Referensi : http://nopi-en.blogspot.com/2018/11/pengujian-silhouette-coefficient.html https://lookmylife.wordpress.com/2011/10/03/metode-silhoutte-coeffisien/","title":"Hasil Running :"},{"location":"index_web_structure/","text":"Web Structure Menggunakan Python Nama : Iis Yuni Harianti NIM : 160411100086 Semester : 6 Mata kuliah : Pengembangan dan Pencarian Web Dosen Pengampu : Mulaab, S.Si., M.Kom Library yang di butuhkan untuk web structure mining untuk memulai melakukan crawl pastikan python yang digunakan sudah memiliki library sebagai berikut : Pandas digunakan untuk mengolah suatu data dan mengolahnya dengan teknik seperti pada SQL. Hanya saja dilakukan pada tabel yang dimuat dari file ke RAM. Pandas adalah spreadsheet namun tidak memiliki GUI dan punya fitur seperti SQL. 1 BeautifulSoup4 digunakan untuk mengambil data berdasarkan tag html. requests digunakan untuk menganbil satu halaman html. Networkx digunakan untuk mengubah kata menjadi kata dasar. Matplotlib digunakan untuk menampilkan memvisualisasikan data data secara 2D atau 3D. 1 import pandas as pd import requests from bs4 import BeautifulSoup import networkx as nx import matplotlib.pyplot as plt Referensi : https://www.codepolitan.com/5-library-python-untuk-data-science-59b774b6cad97","title":"Pengantar"},{"location":"index_web_structure/#web-structure-menggunakan-python","text":"Nama : Iis Yuni Harianti NIM : 160411100086 Semester : 6 Mata kuliah : Pengembangan dan Pencarian Web Dosen Pengampu : Mulaab, S.Si., M.Kom","title":"Web Structure Menggunakan Python"},{"location":"index_web_structure/#library-yang-di-butuhkan-untuk-web-structure-mining","text":"untuk memulai melakukan crawl pastikan python yang digunakan sudah memiliki library sebagai berikut : Pandas digunakan untuk mengolah suatu data dan mengolahnya dengan teknik seperti pada SQL. Hanya saja dilakukan pada tabel yang dimuat dari file ke RAM. Pandas adalah spreadsheet namun tidak memiliki GUI dan punya fitur seperti SQL. 1 BeautifulSoup4 digunakan untuk mengambil data berdasarkan tag html. requests digunakan untuk menganbil satu halaman html. Networkx digunakan untuk mengubah kata menjadi kata dasar. Matplotlib digunakan untuk menampilkan memvisualisasikan data data secara 2D atau 3D. 1 import pandas as pd import requests from bs4 import BeautifulSoup import networkx as nx import matplotlib.pyplot as plt Referensi : https://www.codepolitan.com/5-library-python-untuk-data-science-59b774b6cad97","title":"Library yang di  butuhkan untuk web structure mining"},{"location":"clustering/K-Means/","text":"Clustering Clustering adalah metode penganalisaan data, yang sering dimasukkan sebagai salah satu metode Data Mining, yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu 'wilayah' yang sama dan data dengan karakteristik yang berbeda ke 'wilayah' yang lain. K-Maens K-Maens merupakan salah satu metode yang digunakan untuk melakukan clustering. K-Means Clustering adalah suatu metode penganalisaan data atau metode Data Mining yang melakukan proses pemodelan tanpa supervisi (unsupervised) dan merupakan salah satu metode yang melakukan pengelompokan data dengan sistem partisi. 1 Metode K-Means Clustering berusaha mengelompokkan data yang ada ke dalam beberapa kelompok, dimana data dalam satu kelompok mempunyai karakteristik yang sama satu sama lainnya dan mempunyai karakteristik yang berbeda dengan data yang ada di dalam kelompok yang lain. Metode K-Means Clustering bertujuan untuk meminimalisasikan objective function yang diset dalam proses clustering dengan cara meminimalkan variasi antar data yang ada di dalam suatu cluster dan memaksimalkan variasi dengan data yang ada di cluster lainnya . Data clustering menggunakan metode K-Means Clustering ini secara umum dilakukan dengan algoritma dasar sebagai berikut: Tentukan jumlah cluster Alokasikan data ke dalam cluster secara random Hitung centroid/rata-rata dari data yang ada di masing-masing cluster Alokasikan masing-masing data ke centroid/rata-rata terdekat Kembali ke Step 3, apabila masih ada data yang berpindah cluster atau apabila perubahan nilai centroid, ada yang di atas nilai threshold yang ditentukan atau apabila perubahan nilai pada objective function yang digunakan di atas nilai threshold yang ditentukan Source Code menghitung K-maens dengan penggunakan Libarary kmeans = KMeans ( n_clusters = i , random_state = 0 ) . fit ( fiturBaru ); classnya = kmeans . labels_ Hasil Running : Referensi : https://informatikalogi.com/algoritma-k-means-clustering/","title":"K-means"},{"location":"clustering/K-Means/#clustering","text":"Clustering adalah metode penganalisaan data, yang sering dimasukkan sebagai salah satu metode Data Mining, yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu 'wilayah' yang sama dan data dengan karakteristik yang berbeda ke 'wilayah' yang lain.","title":"Clustering"},{"location":"clustering/K-Means/#k-maens","text":"K-Maens merupakan salah satu metode yang digunakan untuk melakukan clustering. K-Means Clustering adalah suatu metode penganalisaan data atau metode Data Mining yang melakukan proses pemodelan tanpa supervisi (unsupervised) dan merupakan salah satu metode yang melakukan pengelompokan data dengan sistem partisi. 1 Metode K-Means Clustering berusaha mengelompokkan data yang ada ke dalam beberapa kelompok, dimana data dalam satu kelompok mempunyai karakteristik yang sama satu sama lainnya dan mempunyai karakteristik yang berbeda dengan data yang ada di dalam kelompok yang lain. Metode K-Means Clustering bertujuan untuk meminimalisasikan objective function yang diset dalam proses clustering dengan cara meminimalkan variasi antar data yang ada di dalam suatu cluster dan memaksimalkan variasi dengan data yang ada di cluster lainnya . Data clustering menggunakan metode K-Means Clustering ini secara umum dilakukan dengan algoritma dasar sebagai berikut: Tentukan jumlah cluster Alokasikan data ke dalam cluster secara random Hitung centroid/rata-rata dari data yang ada di masing-masing cluster Alokasikan masing-masing data ke centroid/rata-rata terdekat Kembali ke Step 3, apabila masih ada data yang berpindah cluster atau apabila perubahan nilai centroid, ada yang di atas nilai threshold yang ditentukan atau apabila perubahan nilai pada objective function yang digunakan di atas nilai threshold yang ditentukan","title":"K-Maens"},{"location":"clustering/K-Means/#source-code-menghitung-k-maens-dengan-penggunakan-libarary","text":"kmeans = KMeans ( n_clusters = i , random_state = 0 ) . fit ( fiturBaru ); classnya = kmeans . labels_","title":"Source Code menghitung K-maens dengan penggunakan Libarary"},{"location":"clustering/K-Means/#hasil-running","text":"Referensi : https://informatikalogi.com/algoritma-k-means-clustering/","title":"Hasil Running :"},{"location":"preprocessing/seleksi_fitur/","text":"Seleksi Fitur Seleksi fitur adalah salah satu tahapan praproses klasifikasi. Seleksi fitur dilakukan dengan cara memilih fitur - fitur yang relevan yang mempengaruhi hasil klasifikasi. Seleksi fitur digunakan untuk mengurangi dimensi data dan fitur - fitur yang tidak relevan. 1 source code untuk seleksi fitur, clustering dan silhouette def pearsonCalculate ( data , u , v ): i, j is an index atas = 0 ; bawah_kiri = 0 ; bawah_kanan = 0 for k in range ( len ( data )): atas += ( data [ k , u ] - meanFitur [ u ]) * ( data [ k , v ] - meanFitur [ v ]) bawah_kiri += ( data [ k , u ] - meanFitur [ u ]) ** 2 bawah_kanan += ( data [ k , v ] - meanFitur [ v ]) ** 2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas / ( bawah_kiri * bawah_kanan ) def meanF ( data ): meanFitur = [] for i in range ( len ( data [ 0 ])): meanFitur . append ( sum ( data [:, i ]) / len ( data )) return np . array ( meanFitur ) def seleksiFiturPearson ( katadasar , data , threshold ): global meanFitur meanFitur = meanF ( data ) u = 0 while u len ( data [ 0 ]): dataBaru = data [:, : u + 1 ] meanBaru = meanFitur [: u + 1 ] katadasarBaru = katadasar [: u + 1 ] v = u while v len ( data [ 0 ]): if u != v : value = pearsonCalculate ( data , u , v ) if value threshold : dataBaru = np . hstack (( dataBaru , data [:, v ] . reshape ( data . shape [ 0 ], 1 ))) meanBaru = np . hstack (( meanBaru , meanFitur [ v ])) katadasarBaru = np . hstack (( katadasarBaru , katadasar [ v ])) v += 1 data = dataBaru meanFitur = meanBaru katadasar = katadasarBaru if u % 50 == 0 : print ( proses : , u , data . shape ) u += 1 return katadasar , data katadasarBaru , fiturBaru = seleksiFiturPearson ( katadasar , tfidf , 0.8 ) for i in range ( 2 , len ( fiturBaru ) - 1 ): kmeans = KMeans ( n_clusters = i , random_state = 0 ) . fit ( fiturBaru ); classnya = kmeans . labels_ s_avg = silhouette_score ( fiturBaru , classnya , random_state = 0 ) print ( Silhouette untuk , i , cluster adalah , s_avg ) print ( kmeans . labels_ ) print ( proses selesai ) with open ( Anggota_cluster.csv , newline = , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) for i in classnya . reshape ( - 1 , 1 ): employee_writer . writerow ( i ) with open ( Seleksi_Fitur.csv , newline = , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) employee_writer . writerow ([ katadasarBaru . tolist ()]) for i in fiturBaru : employee_writer . writerow ( i ) Hasil Running : Referensi : https://repository.ipb.ac.id/handle/123456789/14020","title":"Seleksi Fitur"},{"location":"preprocessing/seleksi_fitur/#seleksi-fitur","text":"Seleksi fitur adalah salah satu tahapan praproses klasifikasi. Seleksi fitur dilakukan dengan cara memilih fitur - fitur yang relevan yang mempengaruhi hasil klasifikasi. Seleksi fitur digunakan untuk mengurangi dimensi data dan fitur - fitur yang tidak relevan. 1","title":"Seleksi Fitur"},{"location":"preprocessing/seleksi_fitur/#source-code-untuk-seleksi-fitur-clustering-dan-silhouette","text":"def pearsonCalculate ( data , u , v ): i, j is an index atas = 0 ; bawah_kiri = 0 ; bawah_kanan = 0 for k in range ( len ( data )): atas += ( data [ k , u ] - meanFitur [ u ]) * ( data [ k , v ] - meanFitur [ v ]) bawah_kiri += ( data [ k , u ] - meanFitur [ u ]) ** 2 bawah_kanan += ( data [ k , v ] - meanFitur [ v ]) ** 2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas / ( bawah_kiri * bawah_kanan ) def meanF ( data ): meanFitur = [] for i in range ( len ( data [ 0 ])): meanFitur . append ( sum ( data [:, i ]) / len ( data )) return np . array ( meanFitur ) def seleksiFiturPearson ( katadasar , data , threshold ): global meanFitur meanFitur = meanF ( data ) u = 0 while u len ( data [ 0 ]): dataBaru = data [:, : u + 1 ] meanBaru = meanFitur [: u + 1 ] katadasarBaru = katadasar [: u + 1 ] v = u while v len ( data [ 0 ]): if u != v : value = pearsonCalculate ( data , u , v ) if value threshold : dataBaru = np . hstack (( dataBaru , data [:, v ] . reshape ( data . shape [ 0 ], 1 ))) meanBaru = np . hstack (( meanBaru , meanFitur [ v ])) katadasarBaru = np . hstack (( katadasarBaru , katadasar [ v ])) v += 1 data = dataBaru meanFitur = meanBaru katadasar = katadasarBaru if u % 50 == 0 : print ( proses : , u , data . shape ) u += 1 return katadasar , data katadasarBaru , fiturBaru = seleksiFiturPearson ( katadasar , tfidf , 0.8 ) for i in range ( 2 , len ( fiturBaru ) - 1 ): kmeans = KMeans ( n_clusters = i , random_state = 0 ) . fit ( fiturBaru ); classnya = kmeans . labels_ s_avg = silhouette_score ( fiturBaru , classnya , random_state = 0 ) print ( Silhouette untuk , i , cluster adalah , s_avg ) print ( kmeans . labels_ ) print ( proses selesai ) with open ( Anggota_cluster.csv , newline = , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) for i in classnya . reshape ( - 1 , 1 ): employee_writer . writerow ( i ) with open ( Seleksi_Fitur.csv , newline = , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) employee_writer . writerow ([ katadasarBaru . tolist ()]) for i in fiturBaru : employee_writer . writerow ( i )","title":"source code untuk seleksi fitur, clustering dan silhouette"},{"location":"preprocessing/seleksi_fitur/#hasil-running","text":"Referensi : https://repository.ipb.ac.id/handle/123456789/14020","title":"Hasil Running :"},{"location":"textExtention/TF-IDF/","text":"TF-IDF Tf menyatakan jumlah berapa banyak keberadaan suatu kata dalam satu dokumen. 1 IDF (Inverse Document Frequency) merupakan sebuah perhitungan dari bagaimana kata didistribusikan secara luas pada koleksi dokumen yang bersangkutan. IDF menunjukkan hubungan ketersediaan sebuah kata dalam seluruh dokumen. Semakin sedikit jumlah dokumen yang mengandung kata yang dimaksud, maka nilai IDF semakin besar. 2 Rumus IDF : Rumus TF-IDF : source code tf-idf df = list () for d in range ( len ( matrix [ 0 ])): total = 0 for i in range ( len ( matrix )): if matrix [ i ][ d ] != 0 : total += 1 df . append ( total ) idf = list () for i in df : tmp = 1 + log10 ( len ( matrix ) / ( 1 + i )) idf . append ( tmp ) tf = matrix tfidf = [] for baris in range ( len ( matrix )): tampungBaris = [] for kolom in range ( len ( matrix [ 0 ])): tmp = tf [ baris ][ kolom ] * idf [ kolom ] tampungBaris . append ( tmp ) tfidf . append ( tampungBaris ) tfidf = np . array ( tfidf ) print ( tf_idf ) with open ( tf-idf.csv , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) employee_writer . writerow ( katadasar ) for i in tfidf : employee_writer . writerow ( i ) Hasil Running : Referensi : https://rahmadya.com/2014/09/25/term-frequency-dan-invers-document-frequency-tf-idf/ https://informatikalogi.com/term-weighting-tf-idf/","title":"TF-IDF"},{"location":"textExtention/TF-IDF/#tf-idf","text":"Tf menyatakan jumlah berapa banyak keberadaan suatu kata dalam satu dokumen. 1 IDF (Inverse Document Frequency) merupakan sebuah perhitungan dari bagaimana kata didistribusikan secara luas pada koleksi dokumen yang bersangkutan. IDF menunjukkan hubungan ketersediaan sebuah kata dalam seluruh dokumen. Semakin sedikit jumlah dokumen yang mengandung kata yang dimaksud, maka nilai IDF semakin besar. 2 Rumus IDF : Rumus TF-IDF :","title":"TF-IDF"},{"location":"textExtention/TF-IDF/#source-code-tf-idf","text":"df = list () for d in range ( len ( matrix [ 0 ])): total = 0 for i in range ( len ( matrix )): if matrix [ i ][ d ] != 0 : total += 1 df . append ( total ) idf = list () for i in df : tmp = 1 + log10 ( len ( matrix ) / ( 1 + i )) idf . append ( tmp ) tf = matrix tfidf = [] for baris in range ( len ( matrix )): tampungBaris = [] for kolom in range ( len ( matrix [ 0 ])): tmp = tf [ baris ][ kolom ] * idf [ kolom ] tampungBaris . append ( tmp ) tfidf . append ( tampungBaris ) tfidf = np . array ( tfidf ) print ( tf_idf ) with open ( tf-idf.csv , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) employee_writer . writerow ( katadasar ) for i in tfidf : employee_writer . writerow ( i )","title":"source code tf-idf"},{"location":"textExtention/TF-IDF/#hasil-running","text":"Referensi : https://rahmadya.com/2014/09/25/term-frequency-dan-invers-document-frequency-tf-idf/ https://informatikalogi.com/term-weighting-tf-idf/","title":"Hasil Running :"},{"location":"textExtention/VSM/","text":"TextProcessing Pertama Tokenisasi adalah proses untuk membagi teks yang dapat berupa kalimat, paragraf atau dokumen, menjadi token - token / bagian - bagian tertentu. Sebagai contoh, tokenisasi dari kalimat \"Aku baru saja makan bakso pedas\" menghasilkan enam token, yakni: \"Aku\", \"baru\", \"saja\", \"makan\", \"bakso\", \"pedas\". Biasanya, yang menjadi acuan pemisah antar token adalah spasi dan tanda baca. Tokenisasi seringkali dipakai dalam ilmu linguistik dan hasil tokenisasi berguna untuk analisis teks lebih lanjut. 1 Kedua Stop words adalah kata umum (common words) yang biasanya muncul dalam jumlah besar dan dianggap tidak memiliki makna. Stop words umumnya dimanfaatkan dalam task information retrieval. Contoh stop words untuk bahasa Inggris diantaranya \u201cof\u201d, \u201cthe\u201d. Sedangkan untuk bahasa Indonesia diantaranya \u201cyang\u201d, \u201cdi\u201d, \u201cke\u201d. 1 Ketiga Stemmming merupakan salah satu proses dari pembuatan sistem temu kembali, dimana proses stemming akan dilakukan setelah proses filtering. Proses stemming ini membuat term yang ada pada tabel filtering menjadi kata dasar, dengan menghilankan semua imbuhan yang ada pada kata tersebut ( imbuhan meng-, me-, kan-, di- , i, pe, peng-, a-, dll.). 1 Pentingnya stemming dalam proses pembuatan sistem temu kembali yakni dimana saat menghilangkan imbuhan pada sebuah kata menjadi hal yang perlu diperhatikan. Karena dalam proses stemming yang penting yakni terlebih untuk menghilangkan imbuhan pada awalan setelah itu akhiran. Apabila yang dilakukan adalah sebaliknya maka tidak akan ditemukan kata dasar yang tepat dan sesuai dengan kamus bahasa. Dimana dari hasil proses tersebut akan didapatkan sebuah informasi mengenai banyaknya term yang muncul dalam sebuah dokumen setelah dilakukan perhitungan term frequency. 1 Source Code Text Processing factory = StopWordRemoverFactory () stopword = factory . create_stop_word_remover () factory = StemmerFactory () stemmer = factory . create_stemmer () tmp = for i in isi : tmp = tmp + + i hasil = [] for i in tmp . split (): try : if i . isalpha () and ( not i in hasil ) and len ( i ) 1 : # Menghilangkan Kata tidak penting stop = stopword . remove ( i ) if stop != : stem = stemmer . stem ( stop ) hasil . append ( stem ) except : continue katadasar = np . array ( hasil ) source code untuk menyeleksi kata dasar agar sesuai dengan KBBI #KBBI koneksi = sqlite3 . connect ( KBI.db ) cur_kbi = koneksi . execute ( SELECT* from KATA ) def LinearSearch ( kbi , kata ): found = False posisi = 0 while posisi len ( kata ) and not found : if kata [ posisi ] == kbi : found = True posisi = posisi + 1 return found berhasil = [] for kata in cur_kbi : ketemu = LinearSearch ( kata [ 0 ], katadasar ) if ketemu : kata = kata [ 0 ] berhasil . append ( kata ) print ( berhasil ) katadasar = np . array ( berhasil ) VSM Vector Space Model (VSM) digunakan sebagai representasi dari kumpulan dataset dokumen teks. Dokumen dalam Vector Space Model (VSM) berupa matriks yang berisi bobot seluruh kata pada tiap dokumen. Bobot tersebut menyatakan kepentingan atau kontribusi kata terhadap suatu dokumen dan kumpulan dokumen. 2 source code untuk VSM matrix = [] for row in isi : tamp_isi = [] for a in katadasar : tamp_isi . append ( row . lower () . count ( a )) matrix . append ( tamp_isi ) print ( vsm ) with open ( data_matrix.csv , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) employee_writer . writerow ( katadasar ) for i in matrix : employee_writer . writerow ( i ) kata yang sudah terseleksi kemudian di simpan ke dalam csv dalam bentuk matrix. Hasil Running : Referensi : http://pentingnyakesehatananda.blogspot.com/2017/09/pengertian-tokenisasi-stopword-removal.html https://informatikalogi.com/vector-space-model-pengukuran-jarak/","title":"VSM"},{"location":"textExtention/VSM/#textprocessing","text":"Pertama Tokenisasi adalah proses untuk membagi teks yang dapat berupa kalimat, paragraf atau dokumen, menjadi token - token / bagian - bagian tertentu. Sebagai contoh, tokenisasi dari kalimat \"Aku baru saja makan bakso pedas\" menghasilkan enam token, yakni: \"Aku\", \"baru\", \"saja\", \"makan\", \"bakso\", \"pedas\". Biasanya, yang menjadi acuan pemisah antar token adalah spasi dan tanda baca. Tokenisasi seringkali dipakai dalam ilmu linguistik dan hasil tokenisasi berguna untuk analisis teks lebih lanjut. 1 Kedua Stop words adalah kata umum (common words) yang biasanya muncul dalam jumlah besar dan dianggap tidak memiliki makna. Stop words umumnya dimanfaatkan dalam task information retrieval. Contoh stop words untuk bahasa Inggris diantaranya \u201cof\u201d, \u201cthe\u201d. Sedangkan untuk bahasa Indonesia diantaranya \u201cyang\u201d, \u201cdi\u201d, \u201cke\u201d. 1 Ketiga Stemmming merupakan salah satu proses dari pembuatan sistem temu kembali, dimana proses stemming akan dilakukan setelah proses filtering. Proses stemming ini membuat term yang ada pada tabel filtering menjadi kata dasar, dengan menghilankan semua imbuhan yang ada pada kata tersebut ( imbuhan meng-, me-, kan-, di- , i, pe, peng-, a-, dll.). 1 Pentingnya stemming dalam proses pembuatan sistem temu kembali yakni dimana saat menghilangkan imbuhan pada sebuah kata menjadi hal yang perlu diperhatikan. Karena dalam proses stemming yang penting yakni terlebih untuk menghilangkan imbuhan pada awalan setelah itu akhiran. Apabila yang dilakukan adalah sebaliknya maka tidak akan ditemukan kata dasar yang tepat dan sesuai dengan kamus bahasa. Dimana dari hasil proses tersebut akan didapatkan sebuah informasi mengenai banyaknya term yang muncul dalam sebuah dokumen setelah dilakukan perhitungan term frequency. 1","title":"TextProcessing"},{"location":"textExtention/VSM/#source-code-text-processing","text":"factory = StopWordRemoverFactory () stopword = factory . create_stop_word_remover () factory = StemmerFactory () stemmer = factory . create_stemmer () tmp = for i in isi : tmp = tmp + + i hasil = [] for i in tmp . split (): try : if i . isalpha () and ( not i in hasil ) and len ( i ) 1 : # Menghilangkan Kata tidak penting stop = stopword . remove ( i ) if stop != : stem = stemmer . stem ( stop ) hasil . append ( stem ) except : continue katadasar = np . array ( hasil )","title":"Source Code Text Processing"},{"location":"textExtention/VSM/#source-code-untuk-menyeleksi-kata-dasar-agar-sesuai-dengan-kbbi","text":"#KBBI koneksi = sqlite3 . connect ( KBI.db ) cur_kbi = koneksi . execute ( SELECT* from KATA ) def LinearSearch ( kbi , kata ): found = False posisi = 0 while posisi len ( kata ) and not found : if kata [ posisi ] == kbi : found = True posisi = posisi + 1 return found berhasil = [] for kata in cur_kbi : ketemu = LinearSearch ( kata [ 0 ], katadasar ) if ketemu : kata = kata [ 0 ] berhasil . append ( kata ) print ( berhasil ) katadasar = np . array ( berhasil )","title":"source code untuk menyeleksi kata dasar agar sesuai dengan KBBI"},{"location":"textExtention/VSM/#vsm","text":"Vector Space Model (VSM) digunakan sebagai representasi dari kumpulan dataset dokumen teks. Dokumen dalam Vector Space Model (VSM) berupa matriks yang berisi bobot seluruh kata pada tiap dokumen. Bobot tersebut menyatakan kepentingan atau kontribusi kata terhadap suatu dokumen dan kumpulan dokumen. 2","title":"VSM"},{"location":"textExtention/VSM/#source-code-untuk-vsm","text":"matrix = [] for row in isi : tamp_isi = [] for a in katadasar : tamp_isi . append ( row . lower () . count ( a )) matrix . append ( tamp_isi ) print ( vsm ) with open ( data_matrix.csv , mode = w ) as employee_file : employee_writer = csv . writer ( employee_file , delimiter = , , quotechar = , quoting = csv . QUOTE_MINIMAL ) employee_writer . writerow ( katadasar ) for i in matrix : employee_writer . writerow ( i ) kata yang sudah terseleksi kemudian di simpan ke dalam csv dalam bentuk matrix.","title":"source code untuk VSM"},{"location":"textExtention/VSM/#hasil-running","text":"Referensi : http://pentingnyakesehatananda.blogspot.com/2017/09/pengertian-tokenisasi-stopword-removal.html https://informatikalogi.com/vector-space-model-pengukuran-jarak/","title":"Hasil Running :"}]}